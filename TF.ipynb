{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "burning-recipient",
   "metadata": {},
   "source": [
    "### https://github.com/vyomshm/Skip-gram-Word2vec/blob/master/Skip-Gram%2Bword2vec.ipynb - TF\n",
    "\n",
    "https://github.com/rahul1728jha/Word2Vec_Implementation/blob/master/Word_2_Vec.ipynb - Scratch MAIN\n",
    "\n",
    "https://github.com/SeonbeomKim/TensorFlow-Word2vec/blob/master/1.%20Word2vec%20-%20skip-gram/skip-gram.py - TF\n",
    "https://github.com/ujhuyz0110/wrd_emb/blob/master/word2vec_skipgram_medium_v1.ipynb - TF\n",
    "\n",
    "https://github.com/svaderia/SkipGram - TF\n",
    "\n",
    "\n",
    "https://github.com/n0obcoder/Skip-Gram-Model-PyTorch - PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intense-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developed-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "\n",
    "dataset_folder_path = 'data'\n",
    "dataset_filename = 'text8.zip'\n",
    "dataset_name = 'Text8 Dataset'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(dataset_filename):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc=dataset_name) as pbar:\n",
    "        urlretrieve(\n",
    "            'http://mattmahoney.net/dc/text8.zip',\n",
    "            dataset_filename,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(dataset_folder_path):\n",
    "    with zipfile.ZipFile(dataset_filename) as zip_ref:\n",
    "        zip_ref.extractall(dataset_folder_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imported-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/text8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "closed-amber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst']\n"
     ]
    }
   ],
   "source": [
    "words = utils.preprocess(text)\n",
    "print(words[:30])\n",
    "new_text = [\" \".join(words[0:30])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "gentle-prisoner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "forward-reasoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 16680599\n",
      "Unique words: 63641\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "standing-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "assumed-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "np.random.seed(123)\n",
    "\n",
    "def sub_sampling(int_words, threshold):\n",
    "    word_counter = Counter(int_words)\n",
    "    total_count = len(int_words)\n",
    "    freq= {word: count/total_count for word,count in word_counter.items()}\n",
    "    p_drop ={word:(1-np.sqrt(threshold/ freq[word])) for word in word_counter}\n",
    "    trained_words = [word for word in int_words if p_drop[word]<np.random.random()]\n",
    "    return trained_words\n",
    "\n",
    "train_words = sub_sampling(int_words, threshold= 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "through-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-collect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-irish",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
